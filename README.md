# Mistral_From_Scratch
Mistral and Mixtral from scratch with detailed documentation

- Mistral: https://huggingface.co/docs/transformers/en/model_doc/mistral
- Mixtral MoE (Mixtral of Experts): https://huggingface.co/docs/transformers/en/model_doc/mixtral
- RoPE (Rotary Position Embedding): https://arxiv.org/pdf/2104.09864
- RMSNorm (Root Mean Square Layer Normalization): https://arxiv.org/pdf/1910.07467
- MQA (Multi-Query Attention, 1 KV only): https://arxiv.org/abs/1911.02150
- GQA (Grouped-Query Attention): https://arxiv.org/pdf/2305.13245
- KVCache: https://medium.com/@joaolages/kv-caching-explained-276520203249

