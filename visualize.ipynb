{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from mistral_inference.model import *\n",
    "from mistral_inference.moe import *\n",
    "from mistral_inference.lora import *\n",
    "from mistral_inference.generate import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:48:32.287856Z",
     "start_time": "2024-05-26T03:48:31.580454Z"
    }
   },
   "id": "a0f9f87ea8ec6dc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define necessary model arguments\n",
    "args = ModelArgs(\n",
    "    n_layers=4,\n",
    "    head_dim=32,\n",
    "    hidden_dim=512,\n",
    "    n_heads=4,\n",
    "    dim=128,            # embedding dimension for each input token\n",
    "    n_kv_heads=4,\n",
    "    norm_eps=1e-6,\n",
    "    vocab_size=100,     # vocab size (number of possible tokens)\n",
    "    max_batch_size=16,  # maximum batch size\n",
    "    rope_theta=10000.0,\n",
    "    moe=MoeArgs(\n",
    "        num_experts=4,\n",
    "        num_experts_per_tok=2,\n",
    "    ),\n",
    "    lora=LoraArgs(\n",
    "        rank=4,\n",
    "        scaling=2\n",
    "    )\n",
    ")\n",
    "    \n",
    "# Instantiate the transformer directly\n",
    "transformer = Transformer(\n",
    "    args=args,\n",
    "    pipeline_rank=0,       # Assuming single machine, non-distributed\n",
    "    num_pipeline_ranks=1   # Not using pipeline parallelism\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:48:32.836356Z",
     "start_time": "2024-05-26T03:48:32.810084Z"
    }
   },
   "id": "ac91fcc98dc03581"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(100, 128)\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=128, out_features=100, bias=False)\n",
      "  (layers): ModuleDict(\n",
      "    (0): TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wk): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wv): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wo): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (feed_forward): MoeLayer(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x FeedForward(\n",
      "            (w1): LoRALinear(\n",
      "              (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=512, bias=False)\n",
      "              (linear): Linear(in_features=128, out_features=512, bias=False)\n",
      "            )\n",
      "            (w2): LoRALinear(\n",
      "              (lora_A): Linear(in_features=512, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "              (linear): Linear(in_features=512, out_features=128, bias=False)\n",
      "            )\n",
      "            (w3): LoRALinear(\n",
      "              (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=512, bias=False)\n",
      "              (linear): Linear(in_features=128, out_features=512, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (gate): Linear(in_features=128, out_features=4, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wk): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wv): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wo): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (feed_forward): MoeLayer(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x FeedForward(\n",
      "            (w1): LoRALinear(\n",
      "              (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=512, bias=False)\n",
      "              (linear): Linear(in_features=128, out_features=512, bias=False)\n",
      "            )\n",
      "            (w2): LoRALinear(\n",
      "              (lora_A): Linear(in_features=512, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "              (linear): Linear(in_features=512, out_features=128, bias=False)\n",
      "            )\n",
      "            (w3): LoRALinear(\n",
      "              (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=512, bias=False)\n",
      "              (linear): Linear(in_features=128, out_features=512, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (gate): Linear(in_features=128, out_features=4, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wk): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wv): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wo): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (feed_forward): MoeLayer(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x FeedForward(\n",
      "            (w1): LoRALinear(\n",
      "              (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=512, bias=False)\n",
      "              (linear): Linear(in_features=128, out_features=512, bias=False)\n",
      "            )\n",
      "            (w2): LoRALinear(\n",
      "              (lora_A): Linear(in_features=512, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "              (linear): Linear(in_features=512, out_features=128, bias=False)\n",
      "            )\n",
      "            (w3): LoRALinear(\n",
      "              (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=512, bias=False)\n",
      "              (linear): Linear(in_features=128, out_features=512, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (gate): Linear(in_features=128, out_features=4, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wk): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wv): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (wo): LoRALinear(\n",
      "          (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "          (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (feed_forward): MoeLayer(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x FeedForward(\n",
      "            (w1): LoRALinear(\n",
      "              (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=512, bias=False)\n",
      "              (linear): Linear(in_features=128, out_features=512, bias=False)\n",
      "            )\n",
      "            (w2): LoRALinear(\n",
      "              (lora_A): Linear(in_features=512, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=128, bias=False)\n",
      "              (linear): Linear(in_features=512, out_features=128, bias=False)\n",
      "            )\n",
      "            (w3): LoRALinear(\n",
      "              (lora_A): Linear(in_features=128, out_features=4, bias=False)\n",
      "              (lora_B): Linear(in_features=4, out_features=512, bias=False)\n",
      "              (linear): Linear(in_features=128, out_features=512, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (gate): Linear(in_features=128, out_features=4, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(transformer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:48:33.525713Z",
     "start_time": "2024-05-26T03:48:33.521905Z"
    }
   },
   "id": "e5cb2686eb81b6a0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reach here 0 0 \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BufferCache' object has no attribute 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m encoded_prompts \u001B[38;5;241m=\u001B[39m [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m7\u001B[39m], [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m9\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m9\u001B[39m]]\n\u001B[0;32m----> 2\u001B[0m generated_sequences \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoded_prompts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoded_prompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Replace with an appropriate eos_id if available\u001B[39;49;00m\n\u001B[1;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/llm/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/_DataScience/GitHub/Mistral/Mistral_From_Scratch/mistral_inference/generate.py:73\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(encoded_prompts, model, max_tokens, temperature, chunk_size, eos_id)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreach here 0 0 \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, max_prompt_len, chunk_size):\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreach here 1 \u001B[39m\u001B[38;5;124m\"\u001B[39m, s, \u001B[43mcache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmask\u001B[49m)\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;66;03m# Split encoded prompts into chunks of size chunk_size\u001B[39;00m\n\u001B[1;32m     75\u001B[0m     prompt_chunks \u001B[38;5;241m=\u001B[39m [p[s : s \u001B[38;5;241m+\u001B[39m chunk_size] \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m encoded_prompts]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'BufferCache' object has no attribute 'mask'"
     ]
    }
   ],
   "source": [
    "encoded_prompts = [[1, 2, 3, 1, 2, 5, 7], [4, 5, 6, 9, 4, 5, 6, 9]]\n",
    "generated_sequences = generate(\n",
    "    encoded_prompts=encoded_prompts,\n",
    "    model=transformer,\n",
    "    max_tokens=20,\n",
    "    temperature=0.8,\n",
    "    eos_id=None  # Replace with an appropriate eos_id if available\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:48:34.350373Z",
     "start_time": "2024-05-26T03:48:34.211363Z"
    }
   },
   "id": "824dac3c6bade7b3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:46:42.830351Z",
     "start_time": "2024-05-26T03:46:42.825801Z"
    }
   },
   "id": "3910119176319c52"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hshape torch.Size([2048, 128])\n"
     ]
    }
   ],
   "source": [
    "# input_ids need to be a tensor of shape torch.Size([args.dim * args.max_batch_size])\n",
    "# each entry need to be within range [0, args.vocab_size - 1]\n",
    "input = torch.tensor([args.vocab_size - 1] * (args.dim * args.max_batch_size))\n",
    "seqlens = [args.dim] * args.max_batch_size  # Assuming all sequences are of maximum length for simplicity\n",
    "output = transformer(input, seqlens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:46:43.477403Z",
     "start_time": "2024-05-26T03:46:43.337211Z"
    }
   },
   "id": "f16bb75944de3f0e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2048])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:46:44.346507Z",
     "start_time": "2024-05-26T03:46:44.341617Z"
    }
   },
   "id": "89339f97ce6b4c09"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2048, 100])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:46:44.926145Z",
     "start_time": "2024-05-26T03:46:44.921556Z"
    }
   },
   "id": "47a1bd5941f6c15d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "pred = sample(output, temperature=0.8, top_p=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:46:45.454766Z",
     "start_time": "2024-05-26T03:46:45.446390Z"
    }
   },
   "id": "c7aec2613780a99c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2048])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:46:45.873411Z",
     "start_time": "2024-05-26T03:46:45.869982Z"
    }
   },
   "id": "ac0a00b73c53fc2e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:48:14.215468Z",
     "start_time": "2024-05-26T03:48:14.204015Z"
    }
   },
   "id": "31e5fadcabe9d269"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T03:23:17.631480Z",
     "start_time": "2024-05-26T03:23:17.627635Z"
    }
   },
   "id": "267f81c231716ba9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "44cabbacd905449"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9ec0716b145dd468"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ae86bade5b37ff8c"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def visualize_token_flow(tokens, attention_matrix):\n",
    "    dot = Digraph(comment='Token Flow')\n",
    "\n",
    "    # Adding nodes for each token\n",
    "    for i, token in enumerate(tokens):\n",
    "        dot.node(str(i), token)\n",
    "\n",
    "    # Adding edges based on attention scores\n",
    "    # Assuming attention_matrix is a square matrix with dimensions (len(tokens), len(tokens))\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            # Add edges with labels of attention scores\n",
    "            # Thresholding attention to avoid clutter, only show significant attention flows\n",
    "            if attention_matrix[i][j] > 0.1:  # threshold can be adjusted\n",
    "                dot.edge(str(j), str(i), label=f'{attention_matrix[i][j]:.2f}')\n",
    "\n",
    "    print(dot.source)  # optionally print the DOT source code for debugging\n",
    "    dot.render('token_flow', format='png', cleanup=True)\n",
    "    return dot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T02:26:39.365230Z",
     "start_time": "2024-05-26T02:26:39.349491Z"
    }
   },
   "id": "a8ef2857d6149e8a"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Token Flow\n",
      "digraph {\n",
      "\t0 [label=Hello]\n",
      "\t1 [label=world]\n",
      "\t2 [label=this]\n",
      "\t3 [label=is]\n",
      "\t4 [label=a]\n",
      "\t5 [label=test]\n",
      "\t1 -> 0 [label=0.20]\n",
      "\t1 -> 1 [label=0.30]\n",
      "\t2 -> 2 [label=0.50]\n",
      "\t3 -> 2 [label=0.20]\n",
      "\t2 -> 3 [label=0.20]\n",
      "\t3 -> 3 [label=0.60]\n",
      "\t4 -> 4 [label=0.60]\n",
      "\t5 -> 5 [label=0.70]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (0)\n -->\n<!-- Pages: 1 -->\n<svg width=\"460pt\" height=\"131pt\"\n viewBox=\"0.00 0.00 460.20 131.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 127)\">\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-127 456.2,-127 456.2,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"31.2\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"31.2\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Hello</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"31.2\" cy=\"-105\" rx=\"31.4\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"31.2\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">world</text>\n</g>\n<!-- 1&#45;&gt;0 -->\n<g id=\"edge1\" class=\"edge\">\n<title>1&#45;&gt;0</title>\n<path fill=\"none\" stroke=\"black\" d=\"M31.2,-86.8C31.2,-75.16 31.2,-59.55 31.2,-46.24\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"34.7,-46.18 31.2,-36.18 27.7,-46.18 34.7,-46.18\"/>\n<text text-anchor=\"middle\" x=\"43.7\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.20</text>\n</g>\n<!-- 1&#45;&gt;1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.51,-112.83C70.86,-113.27 80.39,-110.66 80.39,-105 80.39,-101.2 76.09,-98.77 69.82,-97.72\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"69.68,-94.21 59.51,-97.17 69.31,-101.2 69.68,-94.21\"/>\n<text text-anchor=\"middle\" x=\"92.89\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.30</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"152.2\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"152.2\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">this</text>\n</g>\n<!-- 2&#45;&gt;2 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M176.73,-112.75C187.71,-113.49 197.2,-110.91 197.2,-105 197.2,-101.03 192.91,-98.56 186.77,-97.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"186.84,-94.09 176.73,-97.25 186.6,-101.09 186.84,-94.09\"/>\n<text text-anchor=\"middle\" x=\"209.7\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.50</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"129.2\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"129.2\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">is</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge5\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M133,-92.07C125.33,-86.17 117.35,-78.31 113.2,-69 109.62,-60.98 111,-51.92 114.11,-43.74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"117.3,-45.19 118.37,-34.64 110.96,-42.22 117.3,-45.19\"/>\n<text text-anchor=\"middle\" x=\"125.7\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.20</text>\n</g>\n<!-- 3&#45;&gt;2 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M135.96,-35.56C138.15,-41.33 140.46,-47.89 142.2,-54 144.27,-61.29 146.07,-69.3 147.56,-76.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"144.15,-77.54 149.44,-86.72 151.02,-76.24 144.15,-77.54\"/>\n<text text-anchor=\"middle\" x=\"157.7\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">0.20</text>\n</g>\n<!-- 3&#45;&gt;3 -->\n<g id=\"edge6\" class=\"edge\">\n<title>3&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M153.73,-25.75C164.71,-26.49 174.2,-23.91 174.2,-18 174.2,-14.03 169.91,-11.56 163.77,-10.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"163.84,-7.09 153.73,-10.25 163.6,-14.09 163.84,-7.09\"/>\n<text text-anchor=\"middle\" x=\"186.7\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.60</text>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"267.2\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"267.2\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n</g>\n<!-- 4&#45;&gt;4 -->\n<g id=\"edge7\" class=\"edge\">\n<title>4&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M291.73,-112.75C302.71,-113.49 312.2,-110.91 312.2,-105 312.2,-101.03 307.91,-98.56 301.77,-97.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"301.84,-94.09 291.73,-97.25 301.6,-101.09 301.84,-94.09\"/>\n<text text-anchor=\"middle\" x=\"324.7\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.60</text>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"382.2\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"382.2\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">test</text>\n</g>\n<!-- 5&#45;&gt;5 -->\n<g id=\"edge8\" class=\"edge\">\n<title>5&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M406.73,-112.75C417.71,-113.49 427.2,-110.91 427.2,-105 427.2,-101.03 422.91,-98.56 416.77,-97.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"416.84,-94.09 406.73,-97.25 416.6,-101.09 416.84,-94.09\"/>\n<text text-anchor=\"middle\" x=\"439.7\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">0.70</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x12f84b2f0>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['Hello', 'world', 'this', 'is', 'a', 'test']\n",
    "# Example attention matrix (normally this should be output from a model)\n",
    "attention_matrix = [\n",
    "    [0.1, 0.2, 0, 0, 0, 0],\n",
    "    [0.1, 0.3, 0, 0, 0, 0],\n",
    "    [0, 0, 0.5, 0.2, 0.1, 0.1],\n",
    "    [0, 0, 0.2, 0.6, 0.1, 0],\n",
    "    [0, 0, 0.1, 0.1, 0.6, 0.1],\n",
    "    [0, 0, 0, 0, 0.1, 0.7]\n",
    "]\n",
    "visualize_token_flow(tokens, attention_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-26T02:43:02.406161Z",
     "start_time": "2024-05-26T02:43:01.650728Z"
    }
   },
   "id": "a64ecaf0dc03c57f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d431a681ffd61d72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
